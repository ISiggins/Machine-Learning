# import the libs
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt

#import the dataset
df = pd.read_csv("churn.csv")


## Data preprocessing phase #start#
# enconding the cateorical data

df.replace(to_replace=('Male', 'Female'),
           value=['0', '1'], inplace=True)

df.replace(to_replace=('Germany', 'Spain', 'France',),
           value=['0', '1', '2'], inplace=True)

# find any nan in the data set
df.isnull().sum()
# this gives us back all 0, so that means there is no missing values in the dataset

# lets see how skew the dataset is
# this gives us the ratio of the target, which is 7963:2037 whiich rounds to a 4:1 ratio
df.groupby('Exited').count()

# set our X and y variables for train/testing
X = df.iloc[:, 3:-1].values
y = df.iloc[:, -1].values

print(X)  # this is just to make sure our encoding was successful

# split the dataset into the training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# we are going to apply feature scaling to avoid data leakage

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#Data preprocessing phase #end#

# Train the ANN
model = tf.keras.models.Sequential()

# add the layers to the model
model.add(tf.keras.layers.Dense(units=6, activation='relu'))  # first layer
model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))  # second layer

# compile the keras model
model.compile(loss='binary_crossentropy',
              optimizer='adam', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=100, batch_size=32)

# evaluate the keras model
_, accuracy = model.evaluate(X_test, y_test)
print('Accuracy: %.2f' % (accuracy*100))

y_pred = model.predict(X_test)
score = model.evaluate(X_test, y_test, verbose=1)
print(score)

# Hyperparameter search with GridSearch

# for the GridSearch we will be searching for the best epochs and batch_size,
# The KFold valuation will be the default value of 3

from keras.wrappers.scikit_learn import KerasClassifier

def build_clf ():
    model = tf.keras.models.Sequential()

    # add the layers to the model
    model.add(tf.keras.layers.Dense(units=6, activation='relu'))  # first layer
    model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))  # second layer


    # compile the keras model
    model.compile(loss='binary_crossentropy',
                  optimizer='adam', metrics=['accuracy'])
    return model
    
# create the model
model=KerasClassifier(build_fn=build_clf)

# define the grid search parameters
batch_size = [10,20,30,40,50]
epochs = [10,25,50,75,100]

# make a dictioinary of the grid search
param_grid = dict(batch_size = batch_size, epochs = epochs)

# Fit the gridsearch
grid = GridSearchCV(estimator = model,param_grid = param_grid, cv =10)
grid_result = grid.fit(X_test,y_test)

# Summarize the results
# Show the results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))


